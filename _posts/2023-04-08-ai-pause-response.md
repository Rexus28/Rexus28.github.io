# Response to the AI Pause Letter

## The Letter and Context

The [AI-pause open
letter](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) calls
for a moratorium on training of AI systems more powerful than GPT-4 for the
next 6 months. The authors want this to be public and verifiable with
governments stepping in apply regulations if that doesn't happen.

AI researchers like Tminit Gebru and Emily Bender, however, have warned of the
issues with Language Models (LM) for years. The problem isn't their potential
for superhuman intelligence, but their ability to produce text people mistake
as meaningful, when in reality the LM has no understanding of communication or
how people interpret its output; i.e., that LMs are "stochastic parrots"
mimicking sequences of text seen in its training data without understanding its
meaning [1].

These models rely on extremely large datasets of text scraped from the
internet. From sites that are inhabited mostly by white men and contain bias
and a hegemonic viewpoint, and LMs can encode that bias and return it
unprompted, perpetuating this problematic viewpoint [1].

In *Weapons of Math Destruction*, Cathy O'Neil called for better understanding
any and all algorithms where stakeholders include people and for better
understanding the values of those impacted [2]. The problem being, any
algorithm can encode the potential bias of its creators, intentional or not.
Without a deeper understanding of the people an algorithm impacts, an
understanding of the algorithms limitations, and better regulations of where
and when these algorithms can be used these tools can cause significant harm
without superhuman intelligence.


## The Rebuttal

The letter is not without its shortcomings. It's pointed more so directly at
OpenAI than anyone else, making it feel more like a targeted response to the
company's fast development and a way to slow their progress while others catch
up. James Grimmelmann, a Cornell University professor, told the Associated
press "...the letter is vague and doesn't take the regulatory problems
seriously" [3]. On top of providing no concrete steps, 6 months is very little
time to figure out policy, the US is still working on the AI Bill of Rights
since before the recent boom in generative AI [4].

The letter focuses on potential future harms, seemingly as both AI hype and
misdirection of current problems [5]. It ignores all the issues Bender and
Gebru discuss in "Stochastic Parrots" with current (and not past) LMs, and all
the issues O'Neil discusses in *Weapons of Math Destruction*. These LMs, and
really any machine learning model, can encode and amplify bias; and they
typically benefit those already in power while marginalizing those without.
The real issue isn't some future hypothetical but current issues with bias in
large datasets, models we don't understand, and providing tools **now** that
can be used for misinformation. The letter isn't wrong, technically, but it
misses the problems we face today in favor of hyping up potential future
problems of AI that sound more threatening.


## What to do?

There are several things that we can start doing now.

- We shouldn't fear monger about AI, but we should educate people about these
  systems, what they are and are not, how they work, and that they are not
  perfect.
- This includes better evaluation of models in various scenarios and
  documentation of what a model is and is not capable of.
- Researchers should also focus on alternative methods that use smaller and
  easier to understand methods that are intuitive. This will require breaking
  the current LM trend.
- We need to reevaluate the whole data collection process and the resulting
  bias in the data these systems are trained on. This means shifting
  towards smaller datasets that can be evaluated and understood what is in
  them.
- Finally, and the most important thing, we need to focus more on the people
  these systems impact via methods like value sensitive design [1].

Any AI tool has the potential to benefit if we take the time to design it
correctly and understand the needs of the people it will impact; and the
potential for harm if we ignore these needs to focus on building bigger and
bigger models.


## References

1. Bender, Emily M., et al. "On the Dangers of Stochastic Parrots: Can Language
   Models Be Too Big?" *Proceedings of the 2021 ACM conference on fairness,
   accountability, and transparency*. 2021.

2. O'neil, Cathy. *Weapons of math destruction: How big data increases
   inequality and threatens democracy*. Crown, 2017.

3. O'Brien, Matt. "Musk, scientists call for halt to AI race sparked by
   ChatGPT". *AP News*. Retrieved 30 March 2023.
   <https://apnews.com/article/artificial-intelligence-chatgpt-risks-petition-elon-musk-steve-wozniak-534f0298d6304687ed080a5119a69962>

4. Knight, Will and Paresh Dave. "In Sudden Alarm, Tech Doyens Call for a Pause
   on ChatGPT". *Wired*. Retrieved 30 March 2023.
   <https://www.wired.com/story/chatgpt-pause-ai-experiments-open-letter/#intcid=_wired-verso-hp-trending_8bc96dcb-0848-486d-b258-b252fa5d7df5_popular4-1>

5. Xiang, Chloe. "The Open Letter to Stop 'Dangerous' AI Race Is a Huge Mess".
   *Vice*. Retrieved 1 April 2023.
   <https://www.vice.com/en/article/qjvppm/the-open-letter-to-stop-dangerous-ai-race-is-a-huge-mess>
