# The Pitfalls of Explaining AI

In a [previous post](https://rexus28.github.io/2022/06/11/AI_knowledge.html) I discussed what it means for a machine learning algorithm to acquire knowledge using an example of classifying dog breeds. But not all applications of AI or automation algorithms [^1] are picking out pictures of animals. Sometimes they are used in cases where they can make serious decisions about people’s lives.

## The perils of black-box algorithms
Imagine having your future determined by an AI. Its decision about who you are as a person shaping your life without a say from you. This was not a hypothetical for Glenn Rodriguez, an inmate at Eastern Correctional Facility in New York, who, despite having a nearly perfect rehabilitation record, was denied parole because of an algorithm (Wexler 2017). Only after his hearing did Rodriguez learn there was an error in his information fed into COMPAS, a risk assessment tool that uses over 100 inputs to score defendants on how likely they are to reoffend (Corbett-Davies et al. 2016). Because the company that designed COMPAS regards it as proprietary it is hidden from scrutiny (a so-called “black-box” algorithm), there was no way for Rodriguez to show the error had an effect or convince anyone to correct it (Wexler 2017). Why would we let an AI have such a significant impact on someone’s life, especially for something they haven’t done yet and just on the prospect that they might?

Rodriguez did eventually get parole, but lost the time he spent waiting. Or take for example Paul Zilly, who was convicted of stealing tools, but entered a plea deal to serve only one year in jail. The judge overturned the deal, however, when he saw Zilly’s COMPAS score and instead sentenced him to two years in state prison and three years probation (Angwin et al. 2016). The tool, Zilly said, “didn’t take into account all the changes he was making in his life” and “Not that I’m innocent, but I just believe people do change.” (Angwin et al. 2016). COMPAS is intended to provide insights for determining parole, probation, and support programs needed to help people from reoffending after release. The developer, Northpointe, even testified that it was not meant for sentencing, but they lapsed any guidance after seeing its use in the courts (Angwin et al. 2016). 

## The ethics of algorithms
There are two ethical dilemmas with the use of COMPAS: 1) the developers have claimed it as a trade secret, shielding the algorithm from scrutiny and providing no way for defendant to question how the score was calculated as in the case with Rodriguez; and 2) the judicial system has been incorrectly incorporating its use into areas it was not intended, such as sentencing in the case with Zilly. 

Judges may be subconsciously affected by the algorithm’s score – a result of [automation bias](https://en.wikipedia.org/wiki/Automation_bias) where humans favor decision from an automated process despite information to the contrary – so their final decision on sentencing maybe be impacted (Stepka 2022). These algorithms cannot make the same judgments as humans and their use lacks transparency, especially when they are hidden as trade secrets. Relying on them for any part of the criminal justice system violates the due process of the defendants as there is little to no chance for defendants to argue the algorithm’s decision (Stepka 2022).  This is a one-two-punch for people already at the mercy of our judicial system. 

## Explainable algorithms and AI
Getting a glimpse of how an algorithm operates can let you see what inputs it relies on. In Rodriguez’s case, this would have shown **if** the typo was the deciding factor. For both Rodriguez’s and Zilly’s cases, understanding the algorithm’s favored inputs would have allowed them to argue why it **could** be wrong or how they have changed. It would also provide judges more context on the algorithm’s output and allow them to better apply their own judgment. But explanation of the inputs does not paint the whole picture of performance.

Another advantage of explainable AI is understanding when they fail. No algorithm is perfect, but knowing when it can or cannot be trusted is important and it can also reveal bias. This is what ProPublica showed when they analyzed 10,000 COMPAS records from Broward County, FL finding it was only 61% accurate when predicting people as high risk of recidivism (or reoffending) within two years of an arrest, but that accuracy dropped to 20% when predicting violent recidivism (Angwin et al. 2016; Larson et al. 2016). Meaning COMPAS was not much better than a coin flip for determining who was likely to reoffend and far less accurate at predicting future violent crimes.

More interesting was how COMPAS failed differently for Black and white defendants. Black defendants were twice as likely to be mislabeled as higher risk (at 45%) than white defendants (23%), and white defendants were more likely to be mislabeled as lower risk (48%) compared to Black defendants (28%) (Angwin et al. 2016; Larson et al. 2016). Essentially, what ProPublica found is that COMPAS failed in a way that benefited white defendants and punished Black defendants. 

ProPublica revealed significant bias in the outcomes of COMPAS and highlighted the importance of understanding algorithmic failures. But, COMPAS does not explicitly use race to make its predictions and most of the 137 input fields are multiple choice or yes-or-no options, only a few are manual inputs for prior convictions and age (Rudin 2018; Corbett-Davies et al. 2016). So, why did ProPublica’s findings show racial bias in the COMPAS scores?

## The pitfalls of explainable AI
First off, COMPAS may be reflecting bias in the data used to create the algorithm. It was “designed by experts based on carefully designed surveys and expertise” (Rudin 2018)[^2], but they still relied on data about rearrests which can be a biased measure, as predominantly Black neighborhoods are more heavily policed resulting in more arrests (Corbett-Davies et al. 2016). Any bias in the data, despite a developer’s best intentions, can get picked up by the algorithm. The problem is exacerbated by hiding the inner workings of COMPAS, as we cannot see what inputs lead to the biased decisions and where to correct them.

ProPublica attempted to quantify what inputs COMPAS is relying on by building a logistic regression model, a type of machine learning model. Essentially building one algorithm they understood in order to predict the outcome of another algorithm they couldn’t get access to. But, this is where ProPublica’s analysis starts to go wrong. Despite being able to model the COMPAS predictions with high-fidelity, they made assumptions about how COMPAS actually works, namely they use race and assume important features, such as age, are linear when COMPAS is most likely non-linear (Rudin 2018, Rudin et al. 2018). This means ProPublica’s model simplifies the relationship age plays when predicting recidivism, so their model might have to rely more other variables, like race, sex, and number of priors. They also include race, which COMPAS does not, meaning ProPublica’s model is not a faithful representation of how COMPAS makes its decisions (Rudin 2018).

Models meant to explain black-box algorithms can mislead users based on how that explanation is created. In a separate study, Lakkaraju and Bastani hid the bias of a black-box algorithm designed to use race and gender for bail decisions. They created multiple explanation models using either desirable features (prior incarcerations and failures to attend hearing), prohibited features (race and gender), or both sets of features. Despite using different features, all explanation models closely matched the black-box with 97-99% fidelity. When shown to law students, the trustworthiness of the black-box model closely matched the ratio of desirable to prohibited features in the explanation model they were shown: 10% for the explanation with only prohibited features, 70% for the explanation that used all features, and 88% for the explanation with only desirable features (Lakkaraju and Bastani 2019). All of these explanations and the black-box have the same outcomes, so what this study shows is: 1) it’s possible to construct several different explanations for a black-box and 2) its possible to influence trust (or lack thereof) in a black-box depending on the features selected for the explanation.

I don’t believe ProPublica was intentionally trying to mislead about how COMPAS makes its decisions. ProPublica’s assumptions to use race in their model, however, make it appear that COMPAS is using race in its decisions when it’s in fact not. Race cannot be a *direct* cause of their outcomes because they do not include it (Rudin 2018). That being said, they do include things that are a proxy for race, so race can be an *indirect* cause for COMPAS’s decisions. Just because COMPAS doesn’t directly use race doesn’t mean there is not some workaround. Even the creator of COMPAS admitted that they use inputs that can be correlated with race, such as “poverty, joblessness and social marginalization”, and that excluding them reduces accuracy (Angwin et al. 2016). These seems to say that COMPAS is beholden to bias data, otherwise they wouldn’t be so concerned with objective measures like accuracy.

## What are the solutions?
ProPublica highlighted the systemic issues in our criminal justice system, but pointing the finger at COMPAS here doesn’t paint the whole picture. Work by Rudin et al. and Fisher et al. show other models fit COMPAS well that **do and do not rely on race**. So what does this all mean for ProPublica’s analysis? It highlights the issues with trying to explain black-box models. This does not absolve COMPAS and Northpointe, however, from any wrongdoing. Had they created a transparent model there would have been several issues resolved and less people made to suffer the consequences.

And therein lies the solution: creating algorithms that are transparent and easy for people to interpret. Adding transparency directly into both the model and the way it’s used is the best direction moving forward. We need to create human understanding of AI algorithms along the entire process: from the developers, to users in the criminal justice system including judges, and even the people impacted like the defendants.

## References
1. Rebecca Wexler. “Opinion \| When a Computer Program Keeps You in Jail.” *New York Times*. 2017 June 13. Accessed 2022 March 8. [https://www.nytimes.com/2017/06/13/opinion/how-computers-are-harming-criminal-justice.html](https://www.nytimes.com/2017/06/13/opinion/how-computers-are-harming-criminal-justice.html)

2. Sam Corbett-Davies, Emma Pierson, Avi Feller and Sharad Goel. “A computer program used for bail and sentencing decisions was labeled biased against Blacks. It’s actually not that clear.” *Washington Post*. October 17, 2016. Accessed 2022 March 9. [https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/](https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/)

3. Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner. “Machine Bias: There’s software used across the country to predict future criminals. And it’s biased against Blacks.” *ProPublica*. May 23, 2016. Accessed 2022 March 8. [https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)

4. Matthew Stepka. “Law Bots: How AI Is Reshaping the Legal Profession.” *American Bar Association*. 2022 March 2. Accessed: 2022 March 9. [https://www.americanbar.org/groups/business_law/publications/blt/2022/02/law-bots/](https://www.americanbar.org/groups/business_law/publications/blt/2022/02/law-bots/)

5. Larson, Jeff, et al. "How we analyzed the COMPAS recidivism algorithm." *ProPublica (5 2016)* 9.1 (2016): 3-3. [https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)

6. Rudin, Cynthia. "Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead." *arXiv preprint arXiv:1811.10154* (2018).

7. Rudin, Cynthia, Caroline Wang, and Beau Coker. "The age of secrecy and unfairness in recidivism prediction." *arXiv preprint arXiv:1811.00731* (2018).

8. Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. "All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously." *J. Mach. Learn. Res*. 20.177 (2019): 1-81.

9. Lakkaraju, Himabindu, and Osbert Bastani. ""How do I fool you?": Manipulating User Trust via Misleading Black Box Explanations." *arXiv preprint arXiv:1911.06473* (2019).


---
[^1]: The terms AI and algorithm are used interchangeably in this post.

[^2]: COMPAS is not a machine or deep learning model (what we typically think of as AI), but it does attempt to automate the human decision making process and that does make it a form of AI.

