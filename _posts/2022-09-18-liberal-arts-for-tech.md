# Modern Technology Gets an "F" in Humanities
In 2014, Amazon developed an Artificial Intelligence (AI) tool that rated the resumes of job applicants to speed up the hiring process and get through all the people competing for a job. The company trained the AI using resumes from current employees to rate new resumes on a scale of 1-5. The tool effectively rated how well the new applicants matched the current ones, but within a year, Amazon discovered the tool was making biased ratings against women. Not only did it favor men, but it was actively penalizing female applicants or those with "woman" or "women" in the resume (such as "women's college") [^1]. This result is not surprising considering the AI was trained on data from current employees of one of the largest companies in an industry dominated by men. 

It might seem like a good idea to use data from current employees and it's not like Amazon intended to create an AI to discriminate against women. Instead, the AI learned the implicit bias within that data and exploited it to make its decision. One of the attorneys from ACLU familiar with the tool, Rachel Goodman, stated "these tools are not eliminating human bias — they are merely laundering it through software."[^2]

Cathy O'Neil describes these destructive algorithmic tools, which she dubs weapons of math destruction (the same name as the title of her book), as opaque, have the ability to scale, and are damaging to people [^3]. Luckily, Amazon had the foresight to stop the project, before it could scale up, claiming that it “was never used by Amazon recruiters to evaluate candidates.” [^1], but there is little oversight to check this claim. 

It's likely Amazon wasn't going to let applicants know why they were rejected or allow outside parties to evaluate the tool. Keeping the inner workings of the AI hidden is what helps secure Amazon's advantage, but it's part of what makes WMDs opaque and potentially harmful to the people it's judging. What makes the Amazon tool, and any WMD for that matter, problematic is that it's impacting people with little regard for their well-being.

## The Problem of Modern Technology
O'Neil covers other destructive algorithmic tools used in hiring, job evaluations, scheduling, and other applications, and one thing they have in common is they abstract away the person who's impacted. They replace people with numbers -- like Amazon rating an applicant on a scale of 1-5, but other examples include predicting how likely someone is to commit or [recommit a crime]({% post_url 2022-06-29-pitfalls-of-explaining-AI %}), to pay back or default on a loan, or to click on online advertising. All these tools attempt to represent a complex human being or their actions in as few numbers as possible.

The way we are taught to solve complex problems in science and engineering is to break things down into smaller, simpler pieces. In engineering we make simplifications and assumptions to make it easier to solve. Physics is like this too, we continually simplify and break problems down to just the study of particles. This works really well in physics, but when we abstract too many of the elements away from other people, we end up with the issues that O'Neil describes in her book.

For example, what makes someone a good teacher? This is a highly subjective question, and one we could spend weeks or months debating, if not years. Even if we did come to an objective criteria, could it be something we can measure? Yes, student performance is important for evaluating teachers, but shouldn't student satisfaction and happiness be part of that too? What about inciting curiosity and motivation in one's students? All of these are essential in blossoming students into lifelong learners.

The trouble is that an algorithm might rely only on student performance to be a proximate measure (aka proxy) for other qualities a good teacher should have. There are some problems, however, that cannot be simplified down into data, especially when they focus on people. There are just too many aspects of what makes a person who they are to measure in data, and attempting to do so robs them of their humanity.

## The Liberal Arts Solution
I'm not saying technology cannot help us solve difficult problems, but rather our approach to using technology is broken for problems that focus on people. Instead of relying so heavily on the latest technology we should turn to the forms of knowledge people have used for centuries to deal with difficult problems. Subjects in the humanities, like philosophy, ethics, and even literature, are more foundational because they deal with the questions of what it means to be human being and have been developed and refined over generations of thinkers. 

Modern philosophy was born during the start of the [Scientific Revolution](https://en.wikipedia.org/wiki/Scientific_Revolution), as long existing notions of science, especially the Ptolemaic (or Earth centered) model of the universe, were proven wrong. People didn't know what to think of the world, and from this uncertainty Descartes started thinking about what we can actually know about the universe. This was not an intellectual exercise, Descartes was trying to form a method for building trust in scientific progress, in light of uncertainty, so it could be used as a base for future knowledge [^4].

Because philosophical problems deal so much with uncertainty, they can provide the tech industry a framework for approaching difficult problems. The whole field of philosophy is built on addressing big, important problems that other disciplines either can't or haven't answered yet. In fact, it was Bertrand Russell that said the goal of philosophy is not to provide definite answers but to provide a way for us to freely think outside the confines of dogma [^5].

What better application than technology, where the traditional forms of problem solving have created discriminating and destructive algorithms when looking at people. A willingness to question the norms and ask difficult questions is required. If the tech industry were more flexible to value the topics of the liberal arts, they could bring about the long-term and societal improvements that they intend to have.


[^1]: Jeffrey Dastin. "Amazon scraps secret AI recruiting tool that showed bias against women". *Reuters*. 2018 Oct 10. [https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-        scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)
[^2]: Rachel Goodman. "Why Amazon’s Automated Hiring Tool Discriminated Against Women". *ACLU*. 2018 Oct 12. [https://www.aclu.org/blog/womens-rights/womens-rights-workplace/why-amazons-automated-hiring-tool-discriminated-against](https://www.aclu.org/news/womens-rights/why-amazons-automated-hiring-tool-discriminated-against)
[^3]: O'neil, Cathy. _Weapons of Math Destruction: How big data increases inequality and threatens democracy_. Broadway books, 2016.
[^4]: Stephen West. "Episode #028 - Descartes pt. 1 - A little context". *Philosophize This!*. [https://www.philosophizethis.org/podcast/descartes-dl6x8]( https://www.philosophizethis.org/podcast/descartes-dl6x8)
[^5]: Russell, Bertrand. _The Problems of Philosophy_. Project Gutenberg, 2009. [https://www.gutenberg.org/files/5827/5827-h/5827-h.htm](https://www.gutenberg.org/files/5827/5827-h/5827-h.htm)

